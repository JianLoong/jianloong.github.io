<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Crawling Reddit using Python and AWS EC2 - Part 1 | Jian</title><meta name=keywords content="reddit,crawler,aws"><meta name=description content="An educational approach to crawling Reddit using Python and AWS EC2 without using Reddit API"><meta name=author content="Jian"><link rel=canonical href=https://jianloong.github.io/posts/redditcrawler/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3299c596a7007118365635c056dd427dace22b7b8c1341fdef6fa6c31359ba10.css integrity="sha256-MpnFlqcAcRg2VjXAVt1CfaziK3uME0H972+mwxNZuhA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://jianloong.github.io/images/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jianloong.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://jianloong.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://jianloong.github.io/images/favicon.ico><link rel=mask-icon href=https://jianloong.github.io/images/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-55422968-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Crawling Reddit using Python and AWS EC2 - Part 1"><meta property="og:description" content="An educational approach to crawling Reddit using Python and AWS EC2 without using Reddit API"><meta property="og:type" content="article"><meta property="og:url" content="https://jianloong.github.io/posts/redditcrawler/"><meta property="og:image" content="https://jianloong.github.io/images/crawler.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-17T21:30:56+11:00"><meta property="article:modified_time" content="2022-10-17T21:30:56+11:00"><meta property="og:site_name" content="Jian"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jianloong.github.io/images/crawler.png"><meta name=twitter:title content="Crawling Reddit using Python and AWS EC2 - Part 1"><meta name=twitter:description content="An educational approach to crawling Reddit using Python and AWS EC2 without using Reddit API"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://jianloong.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Crawling Reddit using Python and AWS EC2 - Part 1","item":"https://jianloong.github.io/posts/redditcrawler/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Crawling Reddit using Python and AWS EC2 - Part 1","name":"Crawling Reddit using Python and AWS EC2 - Part 1","description":"An educational approach to crawling Reddit using Python and AWS EC2 without using Reddit API","keywords":["reddit","crawler","aws"],"articleBody":"Objective A demo results of the project can be seem here. This updates every 12 hours.\nThe objective of this post is to complete a web crawler while trying to learn as much as possible while trying to utilise software engineering knowledge and attempt to use good practices. So, this post would be rather long and it would explain the rationale behind each design decision made. If you are after a “just get it done” method, this post is not suitable for that purpose.\nSo, it is expected for certain elements of this post to be longer and seem to be doing things the more tedious way.\nIt is noted that for every subreddit, the easiest way to obtain data is by using the .json at the URL. For example, if you query the programming subreddit using the URL https://www.reddit.com/r/programming/ it would return the HTML page whereas if you used\nhttps://www.reddit.com/r/programming.json It would return the JSON information. You can do a simple cURL command to obtain the results.\ncurl https://www.reddit.com/r/programming.json The returned json would be as follows\n{ \"kind\": \"Listing\", \"data\": { \"after\": \"t3_y7k9vu\", \"dist\": 25, \"modhash\": \"\", \"geo_filter\": null, \"children\": [ { \"kind\": \"t3\", \"data\": { \"approved_at_utc\": null, \"subreddit\": \"programming\", \"selftext\": \"\", Based on this, information it is possible to avoid using the Reddit API and use HTTP requests to obtain the results.\nProject Architecture The tools of this project is as follows.\nGitHub Amazon EC2 - Handles version control of the project - Handles the automation - Automatic builds from gh-pages will allow easy hosting of static json files - Will run python scripts to crawl for data and update repository graph LR\rA--\u003eB\rSetting up your Python environment A python virtual environment is used to run the Python scripts. This is described further in PEP405.\nThe added benefit of using a virtual environment for this is also so that you do not pollute your pip with needless packages.\ndef create_db() -\u003e None: try: sql = \"\"\" CREATE TABLE IF NOT EXISTS POSTS ( ID INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT NOT NULL, permalink TEXT NOT NULL, name TEXT UNIQUE, created INT NOT NULL, selftext TEXT NOT NULL, nta_count INT NOT NULL, yta_count INT NOT NULL, esh_count INT NOT NULL, nah_count INT NOT NULL, info_count INT NOT NULL ); \"\"\" cur = DB_CONNECTION.cursor() cur.execute(sql) except: exit(1) Summary ","wordCount":"389","inLanguage":"en","image":"https://jianloong.github.io/images/crawler.png","datePublished":"2022-10-17T21:30:56+11:00","dateModified":"2022-10-17T21:30:56+11:00","author":{"@type":"Person","name":"Jian"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jianloong.github.io/posts/redditcrawler/"},"publisher":{"@type":"Organization","name":"Jian","logo":{"@type":"ImageObject","url":"https://jianloong.github.io/images/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jianloong.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://jianloong.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://jianloong.github.io/about/ title=about><span>about</span></a></li><li><a href=https://jianloong.github.io/search/ title=search><span>search</span></a></li><li><a href=https://jianloong.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://jianloong.github.io/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jianloong.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://jianloong.github.io/posts/>Posts</a></div><h1 class=post-title>Crawling Reddit using Python and AWS EC2 - Part 1<sup><span class=entry-isdraft>&nbsp;&nbsp;[draft]</span></sup></h1><div class=post-meta><span title='2022-10-17 21:30:56 +1100 AEDT'>October 17, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;389 words&nbsp;·&nbsp;Jian</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#objective>Objective</a></li><li><a href=#project-architecture>Project Architecture</a></li><li><a href=#setting-up-your-python-environment>Setting up your Python environment</a></li><li><a href=#summary>Summary</a></li></ul></nav></div></details></div><div class=post-content><h2 id=objective>Objective<a hidden class=anchor aria-hidden=true href=#objective>#</a></h2><p>A demo results of the project can be seem here. This updates every 12 hours.</p><p>The objective of this post is to complete a web crawler while trying to learn as much as possible while trying to utilise software engineering knowledge and attempt to use good practices. So, this post would be rather long and it would explain the rationale behind each design decision made. If you are after a &ldquo;just get it done&rdquo; method, this post is not suitable for that purpose.</p><figure><img loading=lazy src=/images/crawler.png></figure><p>So, it is expected for certain elements of this post to be longer and seem to be doing things the more tedious way.</p><p>It is noted that for every subreddit, the easiest way to obtain data is by using the <code>.json</code> at the URL. For example, if you query the programming subreddit using the URL <a href=https://www.reddit.com/r/programming/>https://www.reddit.com/r/programming/</a> it would return the HTML page whereas if you used</p><pre tabindex=0><code>https://www.reddit.com/r/programming.json
</code></pre><p>It would return the JSON information. You can do a simple cURL command to obtain the results.</p><pre tabindex=0><code>curl https://www.reddit.com/r/programming.json
</code></pre><p>The returned json would be as follows</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;kind&#34;</span><span class=p>:</span> <span class=s2>&#34;Listing&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;data&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;after&#34;</span><span class=p>:</span> <span class=s2>&#34;t3_y7k9vu&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;dist&#34;</span><span class=p>:</span> <span class=mi>25</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;modhash&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;geo_filter&#34;</span><span class=p>:</span> <span class=kc>null</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;children&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;kind&#34;</span><span class=p>:</span> <span class=s2>&#34;t3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=nt>&#34;data&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=nt>&#34;approved_at_utc&#34;</span><span class=p>:</span> <span class=kc>null</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=nt>&#34;subreddit&#34;</span><span class=p>:</span> <span class=s2>&#34;programming&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=nt>&#34;selftext&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    
</span></span></code></pre></div><p>Based on this, information it is possible to avoid using the Reddit API and use HTTP requests to obtain the results.</p><h2 id=project-architecture>Project Architecture<a hidden class=anchor aria-hidden=true href=#project-architecture>#</a></h2><p>The tools of this project is as follows.</p><table><thead><tr><th>GitHub</th><th>Amazon EC2</th></tr></thead><tbody><tr><td>- Handles version control of the project</td><td>- Handles the automation</td></tr><tr><td>- Automatic builds from gh-pages will allow easy hosting of static json files</td><td>- Will run python scripts to crawl for data and update repository</td></tr></tbody></table><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.1.7/mermaid.min.js integrity="sha512-1ypa9tdUrJAWv5g28Mb5x0zXaUuI4SBofKff88OGyk5D/oOd4x1IPxYHsx3K81bwBKt8NVUvGgw7TgNZ6PJX2A==" crossorigin=anonymous referrerpolicy=no-referrer></script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{location.reload()}),document.body.className.includes("dark")?mermaid.initialize({startOnLoad:!0,securityLevel:"loose",theme:"dark"}):mermaid.initialize({startOnLoad:!0,securityLevel:"loose",theme:"forest"})</script><div class=mermaid align=center id=mermaid>graph LR
A-->B</div><h2 id=setting-up-your-python-environment>Setting up your Python environment<a hidden class=anchor aria-hidden=true href=#setting-up-your-python-environment>#</a></h2><p>A python virtual environment is used to run the Python scripts. This is described further in <a href=https://peps.python.org/pep-0405/>PEP405</a>.</p><p>The added benefit of using a virtual environment for this is also so that you do not pollute your pip with needless packages.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_db</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>sql</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>                CREATE TABLE IF NOT EXISTS POSTS (
</span></span></span><span class=line><span class=cl><span class=s2>                    ID INTEGER PRIMARY KEY AUTOINCREMENT,
</span></span></span><span class=line><span class=cl><span class=s2>                    title TEXT NOT NULL,
</span></span></span><span class=line><span class=cl><span class=s2>                    permalink TEXT NOT NULL,
</span></span></span><span class=line><span class=cl><span class=s2>                    name TEXT UNIQUE,
</span></span></span><span class=line><span class=cl><span class=s2>                    created INT NOT NULL,
</span></span></span><span class=line><span class=cl><span class=s2>                    selftext TEXT NOT NULL,
</span></span></span><span class=line><span class=cl><span class=s2>                    nta_count INT NOT NULL,
</span></span></span><span class=line><span class=cl><span class=s2>                    yta_count INT NOT NULL,
</span></span></span><span class=line><span class=cl><span class=s2>                    esh_count INT NOT NULL,
</span></span></span><span class=line><span class=cl><span class=s2>                    nah_count INT NOT NULL,
</span></span></span><span class=line><span class=cl><span class=s2>                    info_count INT NOT NULL
</span></span></span><span class=line><span class=cl><span class=s2>                );
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>cur</span> <span class=o>=</span> <span class=n>DB_CONNECTION</span><span class=o>.</span><span class=n>cursor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>cur</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=n>sql</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>exit</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2></div><footer class=post-footer><ul class=post-tags><li><a href=https://jianloong.github.io/tags/reddit/>Reddit</a></li><li><a href=https://jianloong.github.io/tags/crawler/>crawler</a></li><li><a href=https://jianloong.github.io/tags/aws/>aws</a></li></ul><nav class=paginav><a class=prev href=https://jianloong.github.io/posts/vscodeaddons/><span class=title>« Prev</span><br><span>Visual Studio Code extensions</span></a>
<a class=next href=https://jianloong.github.io/posts/pihole/><span class=title>Next »</span><br><span>Setting up PiHole as an ad blocker</span></a></nav></footer><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//jianloonggithubio.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2022 <a href=https://jianloong.github.io/>Jian</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>