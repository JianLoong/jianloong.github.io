[{"content":"Recently, generating images with AI has gotten a lot of traction, so I decided to play around with it.\nIt is easy to get it working as there is a web user interface fork of it on GitHub located here. Basically, it is working out of the box with minimal configuration needed.\nInstead of using the suggested model, I used the \u0026ldquo;Waifu Diffusion\u0026rdquo; model instead located here. Waifu diffusion is \u0026ldquo;a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning\u0026rdquo;. So basically, in a way, it is a diffusion model that is better at the generation of anime images.\nHere are some of the images I generated and their prompts. Some of the prompts here are obtained from simple Google searches and modifications are made to them.\n1girl, brown eyes, beanie cap, black hair, closed mouth, earrings, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, blue shirt\n1girl, brown eyes, beanie cap, black hair, closed mouth, earrings, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\n1 girl, sitting on a chair, wearing school uniform, beanie cap, blue jacket in a classroom wearing glasses, black hair, brown eyes, head shot, high resolution, hyper detailed, portrait, soft lips\ngorgeous young Japanese girl sitting by window with headphones on, wearing blue jacket, soft lips, beach blonde hair, octane render, unreal engine, photograph, realistic skin texture, photorealistic, hyper realism, highly detailed, 85mm portrait photography, award winning, hard rim lighting photography\nIt is actually pretty amazing that it could generate such beautiful images. It is even hard for me to differentiate between ones done by real artist or the ones generated by AI.\nReferences https://github.com/AUTOMATIC1111/stable-diffusion-webui\nhttps://mpost.io/best-100-stable-diffusion-prompts-the-most-beautiful-ai-text-to-image-prompts/\nhttps://huggingface.co/hakurei/waifu-diffusion\n","permalink":"https://jianloong.github.io/posts/stablediffusion/","summary":"Recently, generating images with AI has gotten a lot of traction, so I decided to play around with it.\nIt is easy to get it working as there is a web user interface fork of it on GitHub located here. Basically, it is working out of the box with minimal configuration needed.\nInstead of using the suggested model, I used the \u0026ldquo;Waifu Diffusion\u0026rdquo; model instead located here. Waifu diffusion is \u0026ldquo;a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning\u0026rdquo;.","title":"Generating Anime \"Art\" with Stable Diffusion using the \"Waifu Diffusion\" Model"},{"content":"Introduction A question that is often asked is How fair is the marking process for exams? The reason is that in subjects or units, there can be more than 300 students, and to achieve marking consistency policies are needed. This post here attempts to describe how marking is done according to the policies in place.\nPlease note that the opinions expressed here are my own and I am in no way representing the University. I am merely stating facts about how it is done based on my experience.\nBased on my experience working as both a Sessional Lecturer and Teaching associate since 2016, the process of marking is almost always the same for each unit within the faculty. However, the process and methodology might differ depending on the size of the class and the people who are involved.\nThis post describes the marking of paper-based exams, these days e-exams have simplified the marking process to a very high degree with human errors unlikely to occur.\nMarking Process The process of marking exam papers generally takes place as soon as the teaching team obtains the paper. This can be a few days or up to a week before we receive the papers. Depending on how the unit is structured and if the university has campuses overseas for distance learners, all exam papers are marked at the same location without fail. This ensures the marking quality is consistent across the different campuses.\nDepending on the unit and the size of the class, there might be multiple people involved in the processing of marking. Generally, everyone who is teaching the unit and the person who prepares the exam paper would be involved in the marking process.\nThis is how the process normally starts.\nLet\u0026rsquo;s assume the scenario of, a unit that has 300 students and 5 teaching associates that would be involved in the marking process. Depending, on the structure of the exam paper, often time it would consist of multiple questions and these questions would be divided among the teaching team to be assessed.\nFor instance, the \u0026ldquo;Introduction to Programming\u0026rdquo; exam consists of 10 major questions. These will be a breakdown of how these questions are marked and by whom\nTeaching Associate A - marks questions 1 and 2 Teaching Associate B - marks questions 3 and 4 Teaching Associate C - marks questions 5 and 6 and so on The reason this is needed is that, if multiple different people mark the same question, it would be considered consistent. Most of the time, the teaching associate are people who are doing their PhDs but they could also be from the industry working in that specific field. Thus the allocation of the question to the correct person is very important as well. The main reason for this is because, in the dawn of the internet where knowledge is widely available, it is an accepted fact that there are times, certain answers can be better than the accepted answer.\nBesides that, for each question, there will be also a marking rubric. The marking rubric outlines the number of marks given for each answer. This is a complicated process as each answer and the following description can be in a range of how many are allocated. The construction of this marking rubric which outlines the accepted answer for the questions is generally constructed by the person who prepares the exam paper as well as the teaching team. If the marking rubric goes through a drastic change for the marking process, there might be a need to remark all the papers to ensure the process is fair.\nThis marking rubric will then be updated; if a student answers with an accepted answer which is determined by the team during the marking process.\nSo, in summary,\neach question in an exam is allocated to a single marker. (no matter how many papers it is, even if there are 500 students) a marking rubric created by the teaching team is used to evaluate the answers, this marking rubric can be updated if an answer is accepted and not found in the marking rubric itself if the marking rubric is found to be inconsistent or incorrect, all papers will need to be remarked to ensure fairness Besides that, there are also strict policies on the colour of the pen that is used to mark the paper. The first marking is always done using a certain colour. The reason for this is that if there is a need for a remark, a different coloured pen will be used.\nIs my paper anonymous? Most of the time, the exam papers come to the teaching team in the form of bundles. Each bundle can consist of 10 or 20 papers depending on how it is packed. These bundles are also numbered. On top of the bundle, there will be a list of student IDs as well as their names. So, depending on how you view it, your bundle is anonymous but we do know your student ID number. The majority of the time, this is not a concern of us as it would be too troublesome and tedious for us to look up your name while we are marking the papers, and more importantly, knowing your name has no impact on the marking process at all. So, in short, you can say; it is anonymous to a certain extent. (Of course, we can look you up using your student ID; unless there is some sort of mapping system in place)\nDon\u0026rsquo;t spill coffee on my exam paper Normally, we would avoid taking the exam papers home unless it is also absolutely needed. The reason for this is; there have been incidents where members of the teaching team as well as the professor end up losing the bundles and needing an exam resit to happen. So, it is much more secure for the exam papers to be handled by the University and stored in a safe place.\nMarking continued\u0026hellip; Once the marking for a specific question is done, we would \u0026ldquo;transfer\u0026rdquo; the marks from the bottom of the question itself to the front of paper. (The place where you see boxes and the marks for each question). Here is probably one point where errors could happen as well.\nDepending on how long each question would take the mark, the marking would normally complete within less than a week of receiving the papers. However, this is not the end of the process, as we still need to calculate the marks and make sure there are no human errors.\nFor each question, a different marker would now calculate the marks that are tallied up and make sure it is transferred correctly to the front of the question. The reason this is needed is that; each question has many sub-questions and more often than not, there will be a mistake during the counting of marks. This step is to ensure, the previous marker did not make a calculation mistake or a mistake during the transfer of the mark to the front of the page. So it is\nTeaching Associate B will check questions 1 and 2, to make sure the marks are calculated and transferred to the front correctly. Teaching Associate A will check questions 2 and 3, to do the same process and so on Once this step is done, we move on to the next step where the numbers are transferred to an Excel sheet or Google Sheets. This is done via bundle number as we already know the marks in front of the exam papers are correct.\nReconfirm and reconfirm This excel sheet would normally have the breakdown of marks for each question. This is a very important step; because we need to understand the student\u0026rsquo;s performance of each question. Here, normally the person who inputs the value into the sheet would be a different person that did the last step. So, remember how we had bundles? A different person will check the bundles and transfer these marks to the Excel Sheet. (Basically just to confirm the marks transferred from the front of the exam paper to the Excel Sheet are right). The Excel Sheet would serve as the final student marks in the exam.\nEvaluating the performance of each question Here is what we know so far\nEach question is marked by a single person and marks are transferred to the front of the exam paper. The marks that are in front of the exam paper will be confirmed by another person The marks on the front of the exam paper are correct and transferred to the excel sheet Now, we move on,\nWe are finally almost done but not yet\u0026hellip;\u0026hellip;\nWe now have all the marks for each question on the Excel sheet we like to determine which questions the students are performing and which questions are not doing so well as compared to the other questions. The teaching team as a whole would need to address this issue on why this is the case. This is a very important step; there are times when the question itself is either flawed or inconsistent with the teaching material. Imagine the scenario where all students scored 0 for a question, there is an issue with the question.\nHere are some scenarios if a question is found to be a flawed question\nthe question will be removed from the final total, but students who scored marks there will still be given the marks. So, the final total of the paper could be less than 100 marks for instance 90, but there will be also students that have the potential to score more than 90 because, the question is removed, but the marks are still awarded for it. the entire question will be a bonus question, everyone gets the full mark for that question. This approach is not well-liked as students that spend time attempting the question would feel as though it was not justified and not a preferred approach the question will be left there and scaling of markings will be done toward the end Once the teaching team in combination with the professor has chosen what to do, the marking process will move on. This process is normally done based on what the chief examiner thinks of how should the question should perform. (Each question in an exam would normally attempt to cover a learning objective outlined in the unit guide)\nDo also note that, a question can also perform badly because the person marking the question took a very serious approach, but it would also be consistent as all answers for that question are marked by a single individual.\nOne of the benefits of using Google Sheets or Excel is also that we have an audit trail of everything and the changes to the sheet.\nRemarking Now begins one of the more tedious processes in an exam, the remarking phase.\nWe now have all the marks in the Excel Sheet and we do know the number of failures as well as the near passes. Depending on the Faculty, the P grade is given to a student who achieves a mark of 50 and above, and anything below that is considered NA.\nThe process now is to evaluate everyone who is at the borderline marks to achieve a passing mark. This process is often time done by the lead teaching associate or the person who prepared the paper. So, the marking will be repeated by a different person using the enhanced marking rubric in which the previous markers were.\nThe remark will be done in a different coloured pen to indicate the paper has been remarked.\nIt is very common for a remarked paper to not get any grade change because of the entire process and the way the marking is done. There have been multiple cases where it is impossible to give a student even 1 mark for a student to pass and there is nothing the teaching team could do about it. Because giving a single mark, it would mean, all students have been marked unfairly. So it cannot just happen.\nFlowchart The entire process of it can be summarized into a single flowchart.\ngraph TD\rS(Teaching team receives bundles of exam papers)\rA(Each question is allocated to a single marker)\rB(Accepted answers will be discussed amongst the teaching team)\rB1(Marking will happen and completed)\rC(Marks on the question itself will be transferred to the front page)\rD(A different marker checks if the marks on the\nfront is the same with the one on the question)\rE(These marks on the front will be transferred to Google Sheets)\rF(Marks on the front of the exam paper will be check so that it is consistent with the marks in the Google Sheets)\rG(The performance of each question will be analysed by the teaching team)\rH(Questions that are out of performing badly are evaluated)\rI(Remarks will be done for all papers that fall within the 45 to 49 range)\rI1(Remarks will also happen if there is an inconsistency is found)\rJ(Examiner report will be prepared by the person in charge\nexplaining the reason for either spectacular performance or under-performance by the students)\rK(Final marks will be submitted to the faculty using the sheets provided)\rS--\u003eA\rA--\u003eB\rB--\u003eB1\rB1--\u003eC\rC--\u003eD\rD--\u003eE\rE--\u003eF\rF--\u003eG\rG--\u003eH\rH--\u003eI\rI--\u003eI1\rI1--\u003eJ\rJ--\u003eK\rSummary I think the marking processes used are very fair and consistent. With such a marking process, the high achievers are very obvious because their performance would be consistent for all questions, as all of the markers agreed that all of the questions are answered well.\nAlso, because of how the marking is done, a student with near pass marks oftentimes will not experience a grade change because of how everything is structured as finding even 1 mark out of sympathy and not using the marking rubric would mean All answers to that question would need to be remarked to ensure fairness for all students. So, it is a very rare occurrence it would happen.\nThe conclusion is, exam marking is treated very seriously and with the utmost respect. However, it would finally depend if the teaching team has followed the laid-out procedures set in place.\nAnd also in case you were also asking Does the good-looking person always perform better?, the answer is strictly no, because if proper procedures were followed, the examination marking process is very fair and consistent.\n","permalink":"https://jianloong.github.io/posts/process_of_exam_paper_assessment/","summary":"This post explains the examination marking process so that it is consistent and fair to the students. This is before the age of \u003cem\u003ee-exams\u003c/em\u003e","title":"Process of Exam Papers Assessment"},{"content":"Introduction \u0026ldquo;Pi-hole is a Linux network-level advertisement and Internet tracker blocking application which acts as a DNS sinkhole and optionally a DHCP server, intended for use on a private network. Pi-hole has the ability to block traditional website advertisements as well as advertisements in unconventional places, such as smart TVs and mobile operating system advertisements.\u0026rdquo;\nThis post will explain how I set up my home computing devices to use PiHole. The total setup time for it, is less than 30 minutes. Setting up a RaspberryPi at home is so much easier these days with the new tools provided especially with the Raspberry Pi imager localted here\nThings Needed I made all my purchases from Core Electronics and so far, for me their service and delivery times are top-notch. It also came with some stickers.\nRaspberry Pi 4 Model B 1GB MicroSD card Raspberry Pi 4 Power Supply Besides that I also purchased\nArgon NEO Raspberry Pi 4 Case Micro-HDMI to HDMI Socket Adapter Cable (Not used in the end) Steps The SD card I purchased came with NOOBs so I flashed it Raspbian. Using the RaspberryPi imager makes this step relatively simple. Remember to also configure the advance options\nEnable SSH Set username and password Configure wireless LAN Set locale settings This step is important so that when your RaspberryPi starts, you can ssh into it instead of using a connecting monitor and keyboard to it.\nAfter that step is done, when you start the RaspberryPi, you can connect to it via ssh. To do this, one way is to find out the RaspberryPi IP address for the initial time. There are a few ways you can do this, one way is to use the arp -a on the command line but you can also use an app that is able to scan your WiFi network for devices. The Network Scanner app on the Google PlayStore is an example of an app you can use. The RaspberryPi device will have a network hostname or vendor name as \u0026ldquo;Raspberry Pi\u0026rdquo; Another way without knowing the IP address would be to use the hostname. The hostname for the RaspberryPI is pi.hole as the hostname if you cannot obtain the IP address of the RaspberryPi. So, either way; it would allow you to ssh into the device.\nUsing putty we will connect to the RaspberryPi. You can either use the IP address you found earlier, or use the hostname. Then you will need to provide the username and password you have setup earlier. If you managed to log in it, you should see something like the image below. Now, we can begin installing PiHole itself. The instructions to install it can be found here. Since the installation process is pretty much automated from this step, it should be pretty straightforward. The only hurdle that you can encounter is when it asked you to set up a static IP address.\nIf so, you can set up a static IP address by following the instructions at this site\nAfter you have completed the installation of PiHole, all that is needed to be done is to either set your devices to use PiHole as the DNS server or change the setting on the router itself. I decided to only change the settings on my devices locally as setting it on the router would invade the privacy of my housemates.\nIt is also possible to access the web interface of PiHole. This is how mine looks after running it for a day. I have also added more adlist for PiHole to filter out as well. This includes. https://raw.githubusercontent.com/kboghdady/youTube_ads_4_pi-hole/master/youtubelist.txt (For Youtube ads) https://blocklistproject.github.io/Lists/tracking.txt (For tracking list) Please also remember to go to Tools → Update Gravity so the list is refreshed.\nConclusion It has become very easy to use RaspberryPi because of the availability of online resources and also the better tools we now have. Previously, it was slightly more complicated to enable ``ssh` and on the RaspberryPi as oftentimes, you would need to connect it to a mouse, keyboard as well as monitor for the initial setup but now it is made simple with the imaging tool.\nThe overall time taken to set it up is less than 30 minutes. Also please remember that even though having a PiHole setup is nice, it will also be useful to still run extensions that block ads if you wish to not see ads.\n","permalink":"https://jianloong.github.io/posts/pihole/","summary":"An educational approach to crawling Reddit using Python and AWS EC2 without using Reddit API","title":"Setting up PiHole as an ad blocker"},{"content":"\rThis post is a simple implementation of how to use the VADER sentiment analysis in a paragraph. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a \u0026ldquo;lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\u0026rdquo;\nFor the project page, please go here\nEnter text to be analysed: Sentiment analysis studies the subjective information in an expression, that is, the opinions, appraisals, emotions, or attitudes towards a topic, person or entity. Expressions can be classified as positive, negative, or neutral. For example: I really like the new design of your website!\rRun\nResults NegativePositiveCompound\rReferences Cjhutto CJHUTTO/Vadersentiment: Vader sentiment analysis. vader (valence aware dictionary and sentiment reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains., GitHub. Available at: https://github.com/cjhutto/vaderSentiment (Accessed: October 20, 2022).\nHutto, C.J. \u0026amp; Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n","permalink":"https://jianloong.github.io/posts/vader/","summary":"This post is a simple implementation of how to use the VADER sentiment analysis in a paragraph. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a \u0026ldquo;lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\u0026rdquo;\nFor the project page, please go here\nEnter text to be analysed: Sentiment analysis studies the subjective information in an expression, that is, the opinions, appraisals, emotions, or attitudes towards a topic, person or entity.","title":"Sentiment Analysis using VADER in JavaScript"},{"content":"This blog post will demonstrate a simple use case where the singleton and observable design pattern becomes important.\nLet\u0026rsquo;s imagine if you are in the arcade and there is a Lucky Dip Machine. The reason I like call it the LuckyDipMachine is because it is one of the feature assignments in the Programming Foundation Units in Monash University and it is often times made fun at. However, the solution for assignment itself can be engineered to be better.\nThere is only one Lucky Dip Machine in the arcade. This lucky dip machine has a limited number of items in its inventory. A singleton design pattern will be used to instantiate this class. The reason for a Singleton pattern is so that, there can only ever be one LuckyDipMachine. Multiple uses of the LuckyDipMachine will only deduct items from this instance.\nEveryone in the arcade can observe when the LuckyDipMachine is used. Whenever, a price is won, the observers would know what Prize has been won.\ngraph TD\rA(LuckyDipMachine\nSingleton)\rB(ObserverOne)\rC(ObserverTwo)\rD(User)\rB -- observes--\u003e A\rC -- observes--\u003e A\rD -- uses --\u003e A\rA --fire changes --\u003e B\rA --fire changes --\u003e C\rFig 1. Flowchart Representation of the intended design We start off by creating a really simple Prize class.\npackage me.jianliew; public class Prize { private String name; public Prize(String name) { this.name = name; } public String getName() { return name; } public void setName(String name) { this.name = name; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Prize prize = (Prize) o; return getName().equals(prize.getName()); } @Override public int hashCode() { return getName().hashCode(); } @Override public String toString() { return \u0026#34;Prize{\u0026#34; + \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } Snippet 1. The Prize class.\nOur Prize class is just a plain old Java object.\nHowever, our LuckyDipMachine class is slightly more interesting. Here are the characteristics of it\nIt has a private constructor. It has a static instance of itself. This is done as a class variable. A PropertyChangeSupport as a field to create an observable pattern. A pull method of the LuckyDipMachine indicating whenever the lucky dip machine has been pulled or used. When this happens, it will fire a property change done via calling the firePropertyChange method. The reason it has a private constructor is quite simple, as you do not want people to use the constructor method but the getInstance() method. This is so that the LuckyDipMachine will only ever have an instance of itself.\npackage me.jianliew; import java.beans.PropertyChangeListener; import java.beans.PropertyChangeSupport; import java.util.*; public class LuckyDipMachine { private Map\u0026lt;Prize, Integer\u0026gt; inventory; private static LuckyDipMachine ourInstance = new LuckyDipMachine(); // PropertyChangeSupport is introduced here private PropertyChangeSupport support; public static LuckyDipMachine getInstance() { return ourInstance; } private LuckyDipMachine() { inventory = new HashMap\u0026lt;\u0026gt;(); support = new PropertyChangeSupport(this); fill(); } private void fill(){ Prize p1 = new Prize(\u0026#34;Potato\u0026#34;); Prize p2 = new Prize(\u0026#34;Tomato\u0026#34;); inventory.put(p1,2); inventory.put(p2,2); } public Prize getRandomPrize(){ List\u0026lt;Prize\u0026gt; keysAsArray = new ArrayList\u0026lt;\u0026gt;(inventory.keySet()); Random r = new Random(); return keysAsArray.get(r.nextInt(keysAsArray.size())); } public void pull(){ if (inventory.isEmpty()) return; Prize p = getRandomPrize(); inventory.computeIfPresent(p, (prize, integer) -\u0026gt; inventory.get(prize) - 1); if(inventory.get(p) == 0){ inventory.remove(p); } support.firePropertyChange(\u0026#34;prize\u0026#34;, \u0026#34;\u0026#34;, p); } public int getInventorySize() { int sum = 0; for (Prize p : inventory.keySet()) sum += inventory.get(p); return sum; } public void addPropertyChangeListener(PropertyChangeListener pcl) { support.addPropertyChangeListener(pcl); } public void removePropertyChangeListener(PropertyChangeListener pcl) { support.removePropertyChangeListener(pcl); } @Override public String toString() { return \u0026#34;LuckyDipMachine{\u0026#34; + \u0026#34;inventory=\u0026#34; + inventory + \u0026#39;}\u0026#39;; } } We also have the Observer. The only thing that is needed in this class is for it to implement the PropertyChangeListener. I will also need to have an implementation for the propertyChange.\npackage me.jianliew; import java.beans.PropertyChangeEvent; import java.beans.PropertyChangeListener; import java.util.ArrayList; import java.util.List; // It is needed to implement the interface PropertyChangeListener public class Observer implements PropertyChangeListener { private List\u0026lt;Prize\u0026gt; observedPrizes; public Observer(){ observedPrizes = new ArrayList\u0026lt;\u0026gt;(); } @Override public void propertyChange(PropertyChangeEvent propertyChangeEvent) { Prize p = (Prize) propertyChangeEvent.getNewValue(); this.observedPrizes.add(p); System.out.println(toString()); } @Override public String toString() { return \u0026#34;Observer{\u0026#34; + \u0026#34;observedPrizes=\u0026#34; + observedPrizes + \u0026#39;}\u0026#39;; } } We can then have a simple Test class to see how it works.\rpackage me.jianliew; public class Test { public static void main(String[] args) { Observer o = new Observer(); LuckyDipMachine ldm = LuckyDipMachine.getInstance(); ldm.addPropertyChangeListener(o); System.out.println(ldm.getInventorySize()); ldm.pull(); ldm.pull(); ldm.pull(); LuckyDipMachine ldm2= LuckyDipMachine.getInstance(); System.out.println(ldm2.getInventorySize()); ldm2.pull(); System.out.println(ldm.getInventorySize()); } } The output of it would be as follows.\n// There will be 4 items at the start 4 // On the first pull a random item is returned. The observer observes it. Observer{observedPrizes=[Prize{name=\u0026#39;Tomato\u0026#39;}]} // On the second pull, the observer now sees 2 items in total. Observer{observedPrizes=[Prize{name=\u0026#39;Tomato\u0026#39;}, Prize{name=\u0026#39;Potato\u0026#39;}]} // There will be three items on the third pull. Observer{observedPrizes=[Prize{name=\u0026#39;Tomato\u0026#39;}, Prize{name=\u0026#39;Potato\u0026#39;}, Prize{name=\u0026#39;Tomato\u0026#39;}]} // The second instance of the LuckyDipMachine will still only have 1 item as the LuckyDipMachine is a singleton. 1 // The pull of the second lucky dip machine, still triggers a fire to the observer as it is a singleton. There is no need to register the listener again. Observer{observedPrizes=[Prize{name=\u0026#39;Tomato\u0026#39;}, Prize{name=\u0026#39;Potato\u0026#39;}, Prize{name=\u0026#39;Tomato\u0026#39;}, Prize{name=\u0026#39;Potato\u0026#39;}]} // At the end there will be nothing left in the machine. 0 Lessons from this blog post. It is very important to override both the equals and the hashCode when using maps. The Singleton design pattern has many different approaches. The approach which I have used here is the most simplistic approach and does have certain design concerns. Design is still a subjective matter as it is debatable to put the quantity field in the Prize class itself. The runtime to get a random item from a HashMap would depend on the implementation of it. Pygments and Chroma of the Hugo static website generator are both good but Chroma seems to look nice out of the box for my use case. PropertyChangeSupport is so much easier to use compared to the Observable interface class. computeIfPresent method is pretty cool as it uses a lambda-like feature. References quantities, I. and L., E. (2019). Inventory of objects with item types and quantities. [online] Code Review Stack Exchange. Available at: https://codereview.stackexchange.com/questions/148821/inventory-of-objects-with-item-types-and-quantities [Accessed 3 Nov. 2019]. Baeldung. (2019). Singletons in Java | Baeldung. [online] Available at: https://www.baeldung.com/java-singleton [Accessed 3 Nov. 2019]. Baeldung. (2019). The Observer Pattern in Java | Baeldung. [online] Available at: https://www.baeldung.com/java-observer-pattern [Accessed 3 Nov. 2019]. ","permalink":"https://jianloong.github.io/posts/singletonobserverpattern/","summary":"This blog post will demonstrate a simple use case where the singleton and observable design pattern becomes important.\nLet\u0026rsquo;s imagine if you are in the arcade and there is a Lucky Dip Machine. The reason I like call it the LuckyDipMachine is because it is one of the feature assignments in the Programming Foundation Units in Monash University and it is often times made fun at. However, the solution for assignment itself can be engineered to be better.","title":"Singleton \u0026 Observer Pattern (Java)"},{"content":"\rOne of the most common queries when working with maps is the nearest neighbour query. This blog post will use Voronoi Diagrams to explain more regarding the nearest neighbour query.\nBelow is an example of a Voronoi diagram generated using d3.js. One of the good use cases of a voronoi diagram in real-life applications would be where would be finding a place to build emergency services. This place should have the most number of neighbouring regions. For example, if you click on the region in the Voronoi diagram below, you can see how many regions in which it would consider this region to be its neighbour. The diagram is randomly generated based on a number of points and thus every refresh of this page will show a different Voronoi diagram.\nFig 1. Voronoi Diagram generated with 30 random points\nThe codes to create this diagram are as follows :-\nconst createVoronoi = () =\u0026gt; { const width = 600; const height = 600; const vertices = d3.range(30).map(function (d) { return [Math.random() * width, Math.random() * height]; }); const delaunay = d3.Delaunay.from(vertices); const voronoi = delaunay.voronoi([0, 0, width, height]); let svg = d3.select(\u0026#34;#canvas\u0026#34;).append(\u0026#34;svg\u0026#34;).attr(\u0026#34;viewBox\u0026#34;, `0 0 600 600`); const mesh = svg .append(\u0026#34;path\u0026#34;) .attr(\u0026#34;fill\u0026#34;, \u0026#34;none\u0026#34;) .attr(\u0026#34;stroke\u0026#34;, \u0026#34;#ccc\u0026#34;) .attr(\u0026#34;stroke-width\u0026#34;, 1) .attr(\u0026#34;d\u0026#34;, voronoi.render()); const bounds = svg .append(\u0026#34;path\u0026#34;) .attr(\u0026#34;fill\u0026#34;, \u0026#34;none\u0026#34;) .attr(\u0026#34;stroke\u0026#34;, \u0026#34;#ccc\u0026#34;) .attr(\u0026#34;stroke-width\u0026#34;, 1) .attr(\u0026#34;d\u0026#34;, voronoi.renderBounds()); const points = svg .append(\u0026#34;path\u0026#34;) .attr(\u0026#34;fill\u0026#34;, \u0026#34;black\u0026#34;) .attr(\u0026#34;stroke\u0026#34;, \u0026#34;#ccc\u0026#34;) .attr(\u0026#34;stroke-width\u0026#34;, 1) .attr(\u0026#34;d\u0026#34;, delaunay.renderPoints()); }; Example of a query I will now re-use my data set of point from an earlier blog entry and I will generate the voronoi diagram. Basically, I would use a query point as well to find out the nearest neighbour and the surrounding neighbours. The figure below shows that the d3-delaunay provides a simple functions that allows you to find the point in which is the closest to the query point. This is done by using delaunay.find(). So if you are inside the blue region, your closest point would be the point inside the blue region.\nIt\u0026rsquo;s surrounding neighbours could then be easily obtained once you have this point by using delaunay.neighbor() and passing the result of the first find function. So, the regions which are in teal would be the neighbours of the region in blue. All the other regions would be coloured in green. This simple data structure would allow you to easily obtain the nearest neighbour. However, of course, there is also the importance of the build time, insertion time and removal time as well.\nFig 2. Voronoi Diagram for a NN query.\nThe codes to create this diagram are as follows const queryExample = () =\u0026gt; { let points = [ [40, 74], [34, 118], [41, 87], [44, 93], [39, 104], [32, 96], [47, 122], [42, 71], ]; // Function to transform the points so that it would work on the grid const transform = (point) =\u0026gt; { return [(point[0] / 90) * 500, (point[1] / 180) * 500]; }; let transformed = []; points.forEach((element) =\u0026gt; { transformed.push(transform(element)); }); const delaunay = d3.Delaunay.from(points); const voronoi = delaunay.voronoi([0, 0, 500, 500]); // Call the draw Voronoi function. drawVoronoi(\u0026#34;#exampleQuery\u0026#34;, transformed); }; const drawVoronoi = (id, vertices, color) =\u0026gt; { const width = 500, height = 500; const delaunay = d3.Delaunay.from(vertices); const voronoi = delaunay.voronoi([0, 0, width, height]); let svg = d3.select(id).append(\u0026#34;svg\u0026#34;).attr(\u0026#34;viewBox\u0026#34;, `0 0 500 500`); const mesh = svg .append(\u0026#34;path\u0026#34;) .attr(\u0026#34;fill\u0026#34;, \u0026#34;none\u0026#34;) .attr(\u0026#34;stroke\u0026#34;, \u0026#34;#ccc\u0026#34;) .attr(\u0026#34;stroke-width\u0026#34;, 1) .attr(\u0026#34;d\u0026#34;, voronoi.render()); const bounds = svg .append(\u0026#34;path\u0026#34;) .attr(\u0026#34;fill\u0026#34;, \u0026#34;none\u0026#34;) .attr(\u0026#34;stroke\u0026#34;, \u0026#34;#ccc\u0026#34;) .attr(\u0026#34;stroke-width\u0026#34;, 1) .attr(\u0026#34;d\u0026#34;, voronoi.renderBounds()); // Find the closest point to this coordinate. const ans = delaunay.find(40, 73); const neighbours = delaunay.neighbors(ans); for (const iterator of neighbours) { renderCell(svg, voronoi, iterator, d3.schemeTableau10[3]); } renderCell(svg, voronoi, ans, d3.schemeTableau10[0]); const points = svg .append(\u0026#34;path\u0026#34;) .attr(\u0026#34;fill\u0026#34;, \u0026#34;black\u0026#34;) .attr(\u0026#34;stroke\u0026#34;, \u0026#34;#ccc\u0026#34;) .attr(\u0026#34;stroke-width\u0026#34;, 1) .attr(\u0026#34;d\u0026#34;, delaunay.renderPoints()); }; const renderCell = (svg, voronoi, index, color) =\u0026gt; { svg .append(\u0026#34;path\u0026#34;) .attr(\u0026#34;fill\u0026#34;, color) .attr(\u0026#34;stroke\u0026#34;, \u0026#34;#ccc\u0026#34;) .attr(\u0026#34;stroke-width\u0026#34;, 1) .attr(\u0026#34;d\u0026#34;, voronoi.renderCell(index)); }; Lessons from this blog post. The learning curve for d3.js is pretty insane. Voronoi are pretty easy using d3. Generators can be iterated using the for...of construct. MDN link There is a way to make SVG responsive. Refer this post. ","permalink":"https://jianloong.github.io/posts/voronoi/","summary":"One of the most common queries when working with maps is the nearest neighbour query. This blog post will use Voronoi Diagrams to explain more regarding the nearest neighbour query.\nBelow is an example of a Voronoi diagram generated using d3.js. One of the good use cases of a voronoi diagram in real-life applications would be where would be finding a place to build emergency services. This place should have the most number of neighbouring regions.","title":"k-Nearest Neighbour on Maps"},{"content":"This post is an entry to describe a use case when the Promise.all JavaScript method is needed. The official reference can be found here. This assumes that the reader has a basic understanding of how promises work.\nBefore we start, there is a need to understand how certain RESTful services are structured. For example, the Hacker News API has an end-point called topstories. This end-point however, does not contain any other information besides a list of item IDs. So, if you would like to obtain the top 10 posts including their title, there would be a need to do several GET requests to fetch them all.\nThe output of\ncurl https://hacker-news.firebaseio.com/v0/topstories.json\u0026#34; is\n[ 33256378, 33259379, 33256446, 33257197, 33249215, 33254791, 33251954, 33257300, 33244819, 33228387, 33247681, . . . These IDs will then be used to obtain more information from a different end point.\ncurl https://hacker-news.firebaseio.com/v0/item/33257197.json which would yield\n{ \u0026#34;by\u0026#34;: \u0026#34;walterbell\u0026#34;, \u0026#34;descendants\u0026#34;: 52, \u0026#34;id\u0026#34;: 33257197, \u0026#34;kids\u0026#34;: [ 33257610, 33257643, 33259365, 33257485, 33258257, 33258605, 33257772, 33257557 ], \u0026#34;score\u0026#34;: 110, \u0026#34;time\u0026#34;: 1666149822, \u0026#34;title\u0026#34;: \u0026#34;IDA cybersecurity software provider Hex-Rays acquired\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;story\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://smartfinvc.com/news/smartfin-acquires-leading-cybersecurity-software-provider-hex-rays-together-with-sfpim-and-sriw/\u0026#34; } Flowchart Representation graph TD\rS(Start)\rA(Fetch from /topstories)\rB{Valid?}\rC(Parse all IDs from /topstories)\rD(Create multiple Promises with Fetch using the /items/id end point based on the IDs obtained)\rE(Create the Promise.all)\rF{Valid?}\rG(Results)\rY(Report Error)\rZ(End)\rS--\u003eA\rA--\u003eB\rB-- Valid Response --\u003eC\rB-- Invalid Response --\u003eY\rC--\u003eD\rD--\u003eE\rE--\u003eF\rF-- Invalid Response --\u003eY\rF-- Valid Response --\u003eG\rY--\u003eZ\rG--\u003eZ\rFig 1. Flow Chart of the Promise.all The code example below would demonstrate a situation where the Promise.all becomes useful.\n// First use the top stories end point to retrieve a list of the top stories const getTopStoriesId = () =\u0026gt; { let endPoint = \u0026#34;https://hacker-news.firebaseio.com/v0/topstories.json\u0026#34;; return fetch(endPoint, { mode: \u0026#34;cors\u0026#34; }).then((response) =\u0026gt; response.json()); }; const getItem = (itemNumber) =\u0026gt; { let endPoint = \u0026#34;//hacker-news.firebaseio.com/v0/item/\u0026#34; + itemNumber + \u0026#34;.json\u0026#34;; return fetch(endPoint, { mode: \u0026#34;cors\u0026#34; }).then((response) =\u0026gt; response.json()); } const topStories = () =\u0026gt; getTopStoriesId().then((result) =\u0026gt; { let promiseArray = []; result.forEach((element) =\u0026gt; { promiseArray.push(getItem(element)); }); return Promise.all(promiseArray); }); Code Example of the Promise.all In order to see it in action, you can copy and paste it on the browser console to see how it works.\nLessons from this blog post. The then function returns a promise as well. Fetch is significantly easier to use in comparison to its jQuery counterparts. However, considerations need to be taken into account when using it in static sites that do not have Babel or modernizr. There are a lot of reasons a lot of users decided to create their wrappers around the Hacker News API. Perhaps, it is deemed that their top stories and ending up which does summary could be done differently. But HN itself is a very opinionated community. Mermaid diagrams are useful and break tags can be introduced in them. CORS and JSONP exist. Cors is more modern and easier to use compared to using JSONP. Using jQuery can make the codes very unreadable and creating callback hell easier. References https://stackoverflow.com/questions/38180080/when-to-use-promise-all ","permalink":"https://jianloong.github.io/posts/promiseall/","summary":"This post is an entry to describe a use case when the Promise.all JavaScript method is needed. The official reference can be found here. This assumes that the reader has a basic understanding of how promises work.\nBefore we start, there is a need to understand how certain RESTful services are structured. For example, the Hacker News API has an end-point called topstories. This end-point however, does not contain any other information besides a list of item IDs.","title":"JavaScript Promise All - Parsing Hackernews Stories using Promise.all"},{"content":"This post is best viewed using the light theme.\nThis post uses GA to generate a high-quality solution to the Traveling Salesman Problem.\nTraveling Salesman Problem using Genetic Algorithm This blog post is regarding using a genetic algorithm to solve the Traveling Salesman Problem. In a one-liner, the TSP asks the following question: Given a list of cities and the distances between each pair of the cities, what is the shortest possible route that visits each city and returns to the origin city?\u0026quot;\nThe conditions in this scenario are that no point can be visited twice and it must return to the starting point. The selected starting point here is New York. (The starting point does not matter in this scenario.). There are times, that a point maybe the revisited more than once to achieve a better solution. The number of cities in this scenario is 13. In this specific implementation, it will never visit the same city twice.\nThe inspiration for this post is based on the google OR-Tools found here. This blog post; uses Genetic Algorithm to obtain the answer. It is implemented with a web worker which runs in the browser based on JavaScript.\nI will also reuse the genetic algorithm implementation written for another blog post however with different fitness functions and different cross-over methodologies.\nLocation Coordinates Shorthand New York 40, -74 A Los Angeles 34, -118 B Chicago 41, -87 C Minneapolis 44, -93 D Denver 39, -104 E Dallas 32,-96 F Seattle 47,-122 G Boston 42,-71 H San Francisco 37,-122 I St. Louis 38,-90 J Houston 29,-95 K Phoenix 33,-111 L Salt Lake City 40,-111 M Total number of cities - 13.\nFor the Genetic Algorithm to work, a distance matrix needs to be given to it. This distance matrix is based on the \u0026ldquo;Euclidean Distance\u0026rdquo; and not the road network distance. The distance matrix is obtained from here which has 13 cities in the United States.\nThe Genetic Algorithm Solution Cross Over Method Ordered\rPMX\rSelection Method Tournament\rRandom\rRank\rRoulette Wheel\rPlease click run to see the results based on different cross over and selection methods. You can repeat this for different methodologies\rRun\rIt can be observed that the selection method random tends to not give a good result as it would defeat the purpose of the GA algorithm. The current mutation rate of the GA is set to 0.2 for this purpose. The starting population size is set to 20.\rDue to the nature of GA, each run under the given settings will give a different solution as I have defaulted the number of generations to 500. This includes running with the same cross over methodology and selection methodology.\nThe fitness in general would depend on the cross over methodology. For example, if the roulette wheel methodology is used, it can be observed that the average fitness tends to spike more.\rThe suggested answer based on the Google OR tools is New York -\u003e Boston -\u003e Chicago -\u003e Minneapolis -\u003e Denver -\u003e Salt Lake City -\u003e Seattle -\u003e San Francisco -\u003e Los Angeles -\u003e Phoenix -\u003e Houston -\u003e Dallas -\u003e St Louis -\u003e New York which gives the total distance of 7293 miles which is also the minimal tour length.\nThe GA however does not obtain this solution. It does however, generate a high quality solution really quick.\nLessons from this post The earth is not flat! Mapping putting coordinates using latitude and longitude on a chart; would work in a different way so it displays beautifully. Latitude and longitude need to be swapped.\nMost chart APIs do not let you specify both the x-axis and y-axis at the same time. This is especially true if the chart is able to generate SVG diagrams. SVG diagrams are always nicer and would generally be more responsive at the end of the day.\nYou can use a series graph to draw lines from point to point in the chartist API. However, chartist API does not like the situation where there are two values on the same axis. (So it is not able to draw a straight line on the x-axis because of the nature of a series chart. An example of this is where there is a point on 30,55 and 30,65.\nThere are specific data sets in which people benchmark their TSP solutions.\nThe GA will downgrade into a random search if the mutation rate is too high. However, the mutation rate can always be changed to tailor to the specific use case.\n","permalink":"https://jianloong.github.io/posts/tsp/","summary":"This post is best viewed using the light theme.\nThis post uses GA to generate a high-quality solution to the Traveling Salesman Problem.\nTraveling Salesman Problem using Genetic Algorithm This blog post is regarding using a genetic algorithm to solve the Traveling Salesman Problem. In a one-liner, the TSP asks the following question: Given a list of cities and the distances between each pair of the cities, what is the shortest possible route that visits each city and returns to the origin city?","title":"Traveling Sales Person using Genetic Algorithms"},{"content":"\rThis post is a simple implementation of Genetic Algorithm GA. Here, you would start with a random string and end up with the target string.\nThis post is heavily inspired based on this website. However, I created the codes with a very different methodology to also include newer JavaScript methods using classes and also web worker so it runs behind the scenes.\nThe implementation of it can be seen here\nCross Over Method One Point\rTwo Point\rUniform\rPMX\rSelection Method Tournament\rRandom\rRank\rRoulette Wheel\rTarget String: Run\nGenerationFitnessString\rObservations Using the methodology random crossover at times will not yield results. The reason for this is simple is because if it is random there might not improvement of the child chromosomes. Using a short \u0026ldquo;target\u0026rdquo; string will yield the result faster, as the problem statement would be significantly easier to solve. Lessons from this post The web worker is often cached for a longer period in production/live environments. Users would have a better experience if it is not required for them to do a hard refresh on the browsers. One easy way is to use the best practice to load the web worker in the head. Others suggested to versioning web-workers. The web worker at times; does not like while loops. It would be better if for loops are used instead. The cross-over methodology for GA needs to be implemented with complexity in mind. Using jQuery might not be the best idea as the hide() and show() which manipulates the display either changing to none or block does not work well on mobile browsers. Perhaps not using jQuery would be better. Designing an encoding is very important. For example, in a knapsack problem there are only two choices. So, each item can either be true or false. References Python Easy GA ","permalink":"https://jianloong.github.io/posts/simplegeneticalgo/","summary":"This post is a simple implementation of Genetic Algorithm GA. Here, you would start with a random string and end up with the target string.\nThis post is heavily inspired based on this website. However, I created the codes with a very different methodology to also include newer JavaScript methods using classes and also web worker so it runs behind the scenes.\nThe implementation of it can be seen here","title":"Genetic Algorithm using Web Workers"},{"content":"\rThe word cloud generated here is based on the website hacker news.\nThe reason this post is; is so that it would easier to see the word that appeared the most for the day.\nEven though the use of a word cloud is not exactly a good representation of occurrence, it still looks nice.\nPlease note that the posts here are generated based on the Hacker News API by doing GET requests. So, it is based on their current entries. So, it will be refreshed when the page is reloaded.\n","permalink":"https://jianloong.github.io/posts/20191014_hackernews_word_cloud/","summary":"The word cloud generated here is based on the website hacker news.\nThe reason this post is; is so that it would easier to see the word that appeared the most for the day.\nEven though the use of a word cloud is not exactly a good representation of occurrence, it still looks nice.\nPlease note that the posts here are generated based on the Hacker News API by doing GET requests.","title":"Hacker News Word Cloud "},{"content":"\rSentiment Analysis for the Sub-Reddit \u0026ldquo;HongKong\u0026rdquo; This post will perform sentiment analysis using AFINN. AFINN is a list of words rated for valence rated with an integer between minus five(negative) and plus five (positive). This implementation uses AFINN-en-165. 1\nThis approach however is very naive as it does not build any models to determine the context of the usage of the word itself.\nPlease note that the posts here are generated based on the Reddit website by doing GET requests. So, it is based on their current entries. So, it will be refreshed when the page is reloaded.\nReferences 1. AFINN Sentiment Analysis\nThe codes to achieve it are as follows\nvar stopWords = [ \u0026#39;about\u0026#39;, \u0026#39;after\u0026#39;, \u0026#39;all\u0026#39;, \u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;an\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;another\u0026#39;, \u0026#39;any\u0026#39;, \u0026#39;are\u0026#39;, \u0026#39;as\u0026#39;, \u0026#39;at\u0026#39;, \u0026#39;be\u0026#39;, \u0026#39;because\u0026#39;, \u0026#39;been\u0026#39;, \u0026#39;before\u0026#39;, \u0026#39;being\u0026#39;, \u0026#39;between\u0026#39;, \u0026#39;both\u0026#39;, \u0026#39;but\u0026#39;, \u0026#39;by\u0026#39;, \u0026#39;came\u0026#39;, \u0026#39;can\u0026#39;, \u0026#39;come\u0026#39;, \u0026#39;could\u0026#39;, \u0026#39;did\u0026#39;, \u0026#39;do\u0026#39;, \u0026#39;each\u0026#39;, \u0026#39;for\u0026#39;, \u0026#39;from\u0026#39;, \u0026#39;get\u0026#39;, \u0026#39;got\u0026#39;, \u0026#39;has\u0026#39;, \u0026#39;had\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;have\u0026#39;, \u0026#39;her\u0026#39;, \u0026#39;here\u0026#39;, \u0026#39;him\u0026#39;, \u0026#39;himself\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;how\u0026#39;, \u0026#39;if\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;into\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;it\u0026#39;, \u0026#39;like\u0026#39;, \u0026#39;make\u0026#39;, \u0026#39;many\u0026#39;, \u0026#39;me\u0026#39;, \u0026#39;might\u0026#39;, \u0026#39;more\u0026#39;, \u0026#39;most\u0026#39;, \u0026#39;much\u0026#39;, \u0026#39;must\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;never\u0026#39;, \u0026#39;now\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;only\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;other\u0026#39;, \u0026#39;our\u0026#39;, \u0026#39;out\u0026#39;, \u0026#39;over\u0026#39;, \u0026#39;said\u0026#39;, \u0026#39;same\u0026#39;, \u0026#39;see\u0026#39;, \u0026#39;should\u0026#39;, \u0026#39;since\u0026#39;, \u0026#39;some\u0026#39;, \u0026#39;still\u0026#39;, \u0026#39;such\u0026#39;, \u0026#39;take\u0026#39;, \u0026#39;than\u0026#39;, \u0026#39;that\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;their\u0026#39;, \u0026#39;them\u0026#39;, \u0026#39;then\u0026#39;, \u0026#39;there\u0026#39;, \u0026#39;these\u0026#39;, \u0026#39;they\u0026#39;, \u0026#39;this\u0026#39;, \u0026#39;those\u0026#39;, \u0026#39;through\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;too\u0026#39;, \u0026#39;under\u0026#39;, \u0026#39;up\u0026#39;, \u0026#39;very\u0026#39;, \u0026#39;was\u0026#39;, \u0026#39;way\u0026#39;, \u0026#39;we\u0026#39;, \u0026#39;well\u0026#39;, \u0026#39;were\u0026#39;, \u0026#39;what\u0026#39;, \u0026#39;where\u0026#39;, \u0026#39;which\u0026#39;, \u0026#39;while\u0026#39;, \u0026#39;who\u0026#39;, \u0026#39;with\u0026#39;, \u0026#39;would\u0026#39;, \u0026#39;you\u0026#39;, \u0026#39;your\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;its\u0026#39;, \u0026#39;why\u0026#39; ]; // https://stackoverflow.com/questions/5631422/stop-word-removal-in-javascript let parseResult = (link) =\u0026gt; { const endPoint = \u0026#34;https://reddit.com\u0026#34; + link + \u0026#34;.json?limit=100\u0026amp;jsonp=?\u0026#34;; let replies = \u0026#34;\u0026#34;; let noOfReplies = 0; $.getJSON(endPoint, function(data){ let title = (data[0].data.children[0].data[\u0026#34;title\u0026#34;]); replies = data[1][\u0026#34;data\u0026#34;].children; let url = \u0026#34;https://reddit.com\u0026#34; + link; noOfReplies = replies.length; let repliesText = \u0026#34;\u0026#34;; let result = { \u0026#34;id\u0026#34; : data[0].data.children[0].data[\u0026#34;id\u0026#34;], \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title, \u0026#34;negative\u0026#34;: 0, \u0026#34;neutral\u0026#34;: 0, \u0026#34;positive\u0026#34;: 0 } for (let i = 0; i \u0026lt; noOfReplies; i++) { let reply = replies[i][\u0026#34;data\u0026#34;].body; let score = buildFreq(reply); switch (true){ case score == 0: result[\u0026#34;neutral\u0026#34;] = result[\u0026#34;neutral\u0026#34;] + 1; break; case score \u0026gt; 0: result[\u0026#34;positive\u0026#34;] = result[\u0026#34;positive\u0026#34;] + 1; break; case score \u0026lt; 0: result[\u0026#34;negative\u0026#34;] = result[\u0026#34;negative\u0026#34;] + 1; break; } } if (result[\u0026#34;negative\u0026#34;] == 0 \u0026amp;\u0026amp; result[\u0026#34;positive\u0026#34;] == 0 \u0026amp;\u0026amp; result[\u0026#34;neutral\u0026#34;] == 0) return; showResult(result); }); } let showResult = (jsonResult) =\u0026gt; { let output = \u0026#34;\u0026lt;strong\u0026gt;\u0026#34; + jsonResult[\u0026#34;title\u0026#34;] + \u0026#34;\u0026lt;/strong\u0026gt;\u0026#34;; let out = output + \u0026#34;\u0026lt;p\u0026gt;\u0026lt;a id=\u0026#34; + jsonResult[\u0026#34;id\u0026#34;] + \u0026#34;_link\u0026gt; Click here\u0026lt;/a\u0026gt; to view post in context.\u0026lt;/p\u0026gt;\u0026#34;; $(\u0026#34;.result\u0026#34;).append(\u0026#34;\u0026lt;div class = \u0026#39;shadow\u0026#39;\u0026gt;\u0026#34; + out +\u0026#34;\u0026lt;div class=\u0026#39;\u0026#39; id=\u0026#34; + jsonResult[\u0026#34;id\u0026#34;] + \u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;\u0026#34;); $(\u0026#34;#\u0026#34; + jsonResult[\u0026#34;id\u0026#34;] + \u0026#34;_link\u0026#34;).prop(\u0026#34;href\u0026#34;, jsonResult[\u0026#34;url\u0026#34;]); $(\u0026#34;.result\u0026#34;).append(\u0026#34;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026#34;); let id = \u0026#34;#\u0026#34; + jsonResult[\u0026#34;id\u0026#34;]; const data = { labels: [\u0026#34;Positive\u0026#34;,\u0026#34;Negative\u0026#34;,\u0026#34;Neutral\u0026#34;], datasets: [ { name: \u0026#34;data\u0026#34;, charType: \u0026#34;percentage\u0026#34;, values: [ jsonResult[\u0026#34;positive\u0026#34;], jsonResult[\u0026#34;negative\u0026#34;], jsonResult[\u0026#34;neutral\u0026#34;] ] } ] } const chart = new frappe.Chart(id, { data: data, type: \u0026#39;percentage\u0026#39;, colors: [\u0026#39;#33691e\u0026#39;, \u0026#39;#b71c1c\u0026#39;,\u0026#39;#e8eaf6\u0026#39;] }) } let buildFreq = (repliesText) =\u0026gt; { if (repliesText === undefined) return 0; let convert = repliesText.replace(/[^\\w\\s]/gi, \u0026#39;\u0026#39;).toLowerCase().split(\u0026#34; \u0026#34;); let totalScore = 0; for(let i = 0; i \u0026lt; convert.length; i++) { let currentWord = convert[i]; totalScore += afinn[currentWord] || 0; } //console.log(totalScore); return totalScore; } let getPost = () =\u0026gt; { let result = \u0026#34;\u0026#34;; let entries = []; let endPoint = \u0026#34;https://reddit.com/r/hongkong.json?limit=30\u0026amp;jsonp=?\u0026#34; $.getJSON(endPoint, function(data){ result = data; entries = result[\u0026#34;data\u0026#34;].children; for(let i = 0; i \u0026lt; entries.length; i++){ let link = (entries[i][\u0026#34;data\u0026#34;][\u0026#34;permalink\u0026#34;]); parseResult(link) } }); } getPost(); ","permalink":"https://jianloong.github.io/posts/hk/","summary":"Sentiment Analysis for the Sub-Reddit \u0026ldquo;HongKong\u0026rdquo; This post will perform sentiment analysis using AFINN. AFINN is a list of words rated for valence rated with an integer between minus five(negative) and plus five (positive). This implementation uses AFINN-en-165. 1\nThis approach however is very naive as it does not build any models to determine the context of the usage of the word itself.\nPlease note that the posts here are generated based on the Reddit website by doing GET requests.","title":"Sentiment analysis for the subreddit Hong Kong"},{"content":"\rThe word cloud generated here is based on the /r/programming subreddit for reddit.com\nThe reason this post is made; is so that it would easier to see the word that appeared the most for the day.\nEven though the use of a word cloud is not exactly a good representation of occurrence, it still looks nice.\nPlease note that the posts here are generated based on the Reddit website by doing GET requests. So, it is based on their current entries. So, it will be refreshed when the page is reloaded.\nThe codes to complete the cloud are as follows\n// Based on http://bl.ocks.org/joews/9697914 with modifications. let words = \u0026#34;\u0026#34;; let freq = \u0026#34;\u0026#34;; let arr = \u0026#34;\u0026#34;; var stopWords = [ \u0026#39;about\u0026#39;, \u0026#39;after\u0026#39;, \u0026#39;all\u0026#39;, \u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;an\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;another\u0026#39;, \u0026#39;any\u0026#39;, \u0026#39;are\u0026#39;, \u0026#39;as\u0026#39;, \u0026#39;at\u0026#39;, \u0026#39;be\u0026#39;, \u0026#39;because\u0026#39;, \u0026#39;been\u0026#39;, \u0026#39;before\u0026#39;, \u0026#39;being\u0026#39;, \u0026#39;between\u0026#39;, \u0026#39;both\u0026#39;, \u0026#39;but\u0026#39;, \u0026#39;by\u0026#39;, \u0026#39;came\u0026#39;, \u0026#39;can\u0026#39;, \u0026#39;come\u0026#39;, \u0026#39;could\u0026#39;, \u0026#39;did\u0026#39;, \u0026#39;do\u0026#39;, \u0026#39;each\u0026#39;, \u0026#39;for\u0026#39;, \u0026#39;from\u0026#39;, \u0026#39;get\u0026#39;, \u0026#39;got\u0026#39;, \u0026#39;has\u0026#39;, \u0026#39;had\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;have\u0026#39;, \u0026#39;her\u0026#39;, \u0026#39;here\u0026#39;, \u0026#39;him\u0026#39;, \u0026#39;himself\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;how\u0026#39;, \u0026#39;if\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;into\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;it\u0026#39;, \u0026#39;like\u0026#39;, \u0026#39;make\u0026#39;, \u0026#39;many\u0026#39;, \u0026#39;me\u0026#39;, \u0026#39;might\u0026#39;, \u0026#39;more\u0026#39;, \u0026#39;most\u0026#39;, \u0026#39;much\u0026#39;, \u0026#39;must\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;never\u0026#39;, \u0026#39;now\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;on\u0026#39;, \u0026#39;only\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;other\u0026#39;, \u0026#39;our\u0026#39;, \u0026#39;out\u0026#39;, \u0026#39;over\u0026#39;, \u0026#39;said\u0026#39;, \u0026#39;same\u0026#39;, \u0026#39;see\u0026#39;, \u0026#39;should\u0026#39;, \u0026#39;since\u0026#39;, \u0026#39;some\u0026#39;, \u0026#39;still\u0026#39;, \u0026#39;such\u0026#39;, \u0026#39;take\u0026#39;, \u0026#39;than\u0026#39;, \u0026#39;that\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;their\u0026#39;, \u0026#39;them\u0026#39;, \u0026#39;then\u0026#39;, \u0026#39;there\u0026#39;, \u0026#39;these\u0026#39;, \u0026#39;they\u0026#39;, \u0026#39;this\u0026#39;, \u0026#39;those\u0026#39;, \u0026#39;through\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;too\u0026#39;, \u0026#39;under\u0026#39;, \u0026#39;up\u0026#39;, \u0026#39;very\u0026#39;, \u0026#39;was\u0026#39;, \u0026#39;way\u0026#39;, \u0026#39;we\u0026#39;, \u0026#39;well\u0026#39;, \u0026#39;were\u0026#39;, \u0026#39;what\u0026#39;, \u0026#39;where\u0026#39;, \u0026#39;which\u0026#39;, \u0026#39;while\u0026#39;, \u0026#39;who\u0026#39;, \u0026#39;with\u0026#39;, \u0026#39;would\u0026#39;, \u0026#39;you\u0026#39;, \u0026#39;your\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;its\u0026#39;, \u0026#39;why\u0026#39; ]; // https://stackoverflow.com/questions/5631422/stop-word-removal-in-javascript function remove_stopwords(str) { res = [] words = str.split(\u0026#39; \u0026#39;) for(i=0;i\u0026lt;words.length;i++) { if(!stopWords.includes(words[i])) { res.push(words[i]) } } return(res.join(\u0026#39; \u0026#39;)) } //Simple animated example of d3-cloud - https://github.com/jasondavies/d3-cloud //Based on https://github.com/jasondavies/d3-cloud/blob/master/examples/simple.html // Encapsulate the word cloud functionality function wordCloud(selector) { var fill = d3.scale.category20(); //Construct the word cloud\u0026#39;s SVG element var svg = d3.select(selector).append(\u0026#34;svg\u0026#34;) .attr(\u0026#34;viewBox\u0026#34;, `0 0 800 800`) .append(\u0026#34;g\u0026#34;) .attr(\u0026#34;transform\u0026#34;, \u0026#34;translate(400,400)\u0026#34;); //Draw the word cloud function draw(words) { var cloud = svg.selectAll(\u0026#34;g text\u0026#34;) .data(words, function(d) { return d.text; }) //Entering words cloud.enter() .append(\u0026#34;text\u0026#34;) .style(\u0026#34;font-family\u0026#34;, \u0026#34;Impact\u0026#34;) .style(\u0026#34;fill\u0026#34;, function(d, i) { return fill(i); }) .attr(\u0026#34;text-anchor\u0026#34;, \u0026#34;middle\u0026#34;) .attr(\u0026#39;font-size\u0026#39;, 1) .text(function(d) { return d.text; }); //Entering and existing words cloud .transition() .duration(600) .style(\u0026#34;font-size\u0026#34;, function(d) { return d.size + \u0026#34;px\u0026#34;; }) .attr(\u0026#34;transform\u0026#34;, function(d) { return \u0026#34;translate(\u0026#34; + [d.x, d.y] + \u0026#34;)rotate(\u0026#34; + d.rotate + \u0026#34;)\u0026#34;; }) .style(\u0026#34;fill-opacity\u0026#34;, 1); //Exiting words cloud.exit() .transition() .duration(200) .style(\u0026#39;fill-opacity\u0026#39;, 1e-6) .attr(\u0026#39;font-size\u0026#39;, 1) .remove(); } //Use the module pattern to encapsulate the visualisation code. We\u0026#39;ll // expose only the parts that need to be public. return { //Recompute the word cloud for a new set of words. This method will // asycnhronously call draw when the layout has been computed. //The outside world will need to call this function, so make it part // of the wordCloud return value. update: function(words) { d3.layout.cloud().size([800, 800]) .words(words) .padding(5) .rotate(function() { return ~~(Math.random() * 2) * 90; }) .font(\u0026#34;Impact\u0026#34;) .fontSize(function(d) { return d.size; }) .on(\u0026#34;end\u0026#34;, draw) .start(); } } } //Prepare one of the sample sentences by removing punctuation, // creating an array of words and computing a random size attribute. function getWords(i) { arr = words.split(\u0026#34; \u0026#34;); let freq = calculateFrequency(arr); return buildResult(arr); } //This method tells the word cloud to redraw with a new set of words. //In reality the new words would probably come from a server request, // user input or some other source. function showNewWords(vis, i) { i = i || 0; vis.update(getWords(i ++ % words.length)) } //Create a new instance of the word cloud visualisation. var myWordCloud = wordCloud(\u0026#39;#cloud\u0026#39;); function calculateFrequency(arr) { var a = [], b = [], prev; arr.sort(); for ( var i = 0; i \u0026lt; arr.length; i++ ) { if ( arr[i] !== prev ) { a.push(arr[i]); b.push(1); } else { b[b.length-1]++; } prev = arr[i]; } return [a,b]; } function buildResult(arr){ let resultArr = []; let sum = 0; let total = freq[0].length; for(let i = 0; i \u0026lt; total; i++) resultArr.push({ text: freq[0][i], size: freq[1][i] }); let sorted = resultArr.sort( (a,b) =\u0026gt; b.size - a.size); sorted = sorted.slice(0, 50); for(let i = 0; i \u0026lt; sorted.length; i++) sum += sorted[i].size; resultArr = []; for(let i = 0; i \u0026lt; sorted.length; i++) resultArr.push({ text: sorted[i][\u0026#34;text\u0026#34;], size: (sorted[i][\u0026#34;size\u0026#34;] / sum) * 60 + 50 }); return resultArr; } let getPost = () =\u0026gt; { let result = \u0026#34;\u0026#34;; let endPoint = \u0026#34;https://reddit.com/r/programming.json?limit=1000\u0026amp;jsonp=?\u0026#34; $.getJSON(endPoint, function(data){ result = data; let entries = result[\u0026#34;data\u0026#34;].children; for(let i = 0; i \u0026lt; entries.length; i++){ let link = (entries[i][\u0026#34;data\u0026#34;][\u0026#34;title\u0026#34;]); words += \u0026#34; \u0026#34; + (entries[i][\u0026#34;data\u0026#34;][\u0026#34;title\u0026#34;]); } words = words.replace(/[^\\w\\s]/gi, \u0026#39;\u0026#39;); words = words.replace(/\\d/g, \u0026#39;\u0026#39;); words = remove_stopwords(words.toLowerCase()); arr = words.split(\u0026#34; \u0026#34;); freq = calculateFrequency(arr); showNewWords(myWordCloud); }); } getPost(); ","permalink":"https://jianloong.github.io/posts/2019/reddit_word_cloud/","summary":"The word cloud generated here is based on the /r/programming subreddit for reddit.com\nThe reason this post is made; is so that it would easier to see the word that appeared the most for the day.\nEven though the use of a word cloud is not exactly a good representation of occurrence, it still looks nice.\nPlease note that the posts here are generated based on the Reddit website by doing GET requests.","title":"Reddit Word Cloud for the subreddit programming"},{"content":"\rThe charts here are visualisation of the Am I the Asshole? subreddit which can be found here\nThe subreddit describes itself as\u0026hellip;\nA catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if you were wrong in an argument that\u0026rsquo;s been bothering you. Tell us about any non-violent conflict you have experienced; give us both sides of the story, and find out if you\u0026rsquo;re right, or you\u0026rsquo;re the asshole. See our Best Of \u0026ldquo;Most Controversial\u0026rdquo; at /r/AITAFiltered!\nPlease note that the posts here are generated based on the Reddit website by doing GET requests. So, it is based on their current entries. So, these are live data obtained from the site and not static data hardcoded in the website.\nAbbreviation Meaning YTA You\u0026rsquo;re the A-hole NTA Not the A-hole ESH Everyone sucks here NAH No A-holes here INFO Not Enough Info Fig 1. Abbreviation Table for the Subreddit Posts let summary = []; let parseResult = (link) =\u0026gt; { const endPoint = \u0026#34;https://reddit.com\u0026#34; + link + \u0026#34;.json?limit=80\u0026amp;jsonp=?\u0026#34;; let replies = \u0026#34;\u0026#34;; $.getJSON(endPoint, function(data){ let title = (data[0].data.children[0].data[\u0026#34;title\u0026#34;]); replies = data[1][\u0026#34;data\u0026#34;].children; let url = \u0026#34;https://reddit.com\u0026#34; + link; let noOfReplies = replies.length; let countNTAAppearance = 0; let nta = \u0026#34;NTA\u0026#34;; let countYTAAppearance = 0; let countESHAppearance = 0; let countNAHAppearance = 0; let countINFOAppearance = 0; for (let i = 0; i \u0026lt; noOfReplies; i++) { let reply = replies[i][\u0026#34;data\u0026#34;].body; if (reply == undefined) return; countNTAAppearance += (reply.match(/NTA/g) || []).length; countYTAAppearance += (reply.match(/YTA/g) || []).length; countESHAppearance += (reply.match(/ESH/g) || []).length; countNAHAppearance += (reply.match(/NAH/g) || []).length; countINFOAppearance += (reply.match(/INFO/g) || []).length; } let jsonResult = { \u0026#34;id\u0026#34; : data[0].data.children[0].data[\u0026#34;id\u0026#34;], \u0026#34;url\u0026#34;: url, \u0026#34;title\u0026#34;: title, \u0026#34;countNTAAppearance\u0026#34;: countNTAAppearance, \u0026#34;countYTAAppearance\u0026#34;: countYTAAppearance, \u0026#34;countESHAppearance\u0026#34; : countESHAppearance, \u0026#34;countNAHAppearance\u0026#34; : countNAHAppearance, \u0026#34;countINFOAppearance\u0026#34; : countINFOAppearance, } summary.push(jsonResult); showResult(jsonResult); }); } let showResult = (jsonResult) =\u0026gt; { let output = \u0026#34;\u0026lt;strong\u0026gt;\u0026#34; + jsonResult[\u0026#34;title\u0026#34;] + \u0026#34;\u0026lt;/strong\u0026gt;\u0026#34;; let out = output + \u0026#34;\u0026lt;p\u0026gt;\u0026lt;a id=\u0026#34; + jsonResult[\u0026#34;id\u0026#34;] + \u0026#34;_link\u0026gt; Click here\u0026lt;/a\u0026gt; to view post in context.\u0026lt;/p\u0026gt;\u0026#34;; $(\u0026#34;.result\u0026#34;).append(\u0026#34;\u0026lt;div class = \u0026#39;shadow\u0026#39;\u0026gt;\u0026#34; + out +\u0026#34;\u0026lt;div class=\u0026#39;\u0026#39; id=\u0026#34; + jsonResult[\u0026#34;id\u0026#34;] + \u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;\u0026#34;); $(\u0026#34;#\u0026#34; + jsonResult[\u0026#34;id\u0026#34;] + \u0026#34;_link\u0026#34;).prop(\u0026#34;href\u0026#34;, jsonResult[\u0026#34;url\u0026#34;]); $(\u0026#34;.result\u0026#34;).append(\u0026#34;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026#34;); let id = \u0026#34;#\u0026#34; + jsonResult[\u0026#34;id\u0026#34;]; const data = { labels: [\u0026#34;NTA\u0026#34;,\u0026#34;YTA\u0026#34;,\u0026#34;ESH\u0026#34;,\u0026#34;NAH\u0026#34;,\u0026#34;INFO\u0026#34;], datasets: [ { name: \u0026#34;data\u0026#34;, charType: \u0026#39;percentage\u0026#39;, values: [ jsonResult[\u0026#34;countNTAAppearance\u0026#34;], jsonResult[\u0026#34;countYTAAppearance\u0026#34;], jsonResult[\u0026#34;countESHAppearance\u0026#34;], jsonResult[\u0026#34;countNAHAppearance\u0026#34;], jsonResult[\u0026#34;countINFOAppearance\u0026#34;], ] } ] } const chart = new frappe.Chart(id, { data: data, type: \u0026#39;percentage\u0026#39;, colors: [\u0026#39;#33691e\u0026#39;, \u0026#39;#b71c1c\u0026#39;, \u0026#39;#f47e17\u0026#39;,\u0026#39;#1a237e\u0026#39;,\u0026#39;#e8eaf6\u0026#39;] }) } let getPost = () =\u0026gt; { let result = \u0026#34;\u0026#34;; let entries = []; let endPoint = \u0026#34;https://reddit.com/r/amitheasshole.json?limit=50\u0026amp;jsonp=?\u0026#34; $.getJSON(endPoint, function(data){ result = data; entries = result[\u0026#34;data\u0026#34;].children; for(let i = 0; i \u0026lt; entries.length; i++){ let link = (entries[i][\u0026#34;data\u0026#34;][\u0026#34;permalink\u0026#34;]); parseResult(link) } }); } getPost(); ","permalink":"https://jianloong.github.io/posts/amitheasshole/","summary":"The charts here are visualisation of the Am I the Asshole? subreddit which can be found here\nThe subreddit describes itself as\u0026hellip;\nA catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if you were wrong in an argument that\u0026rsquo;s been bothering you. Tell us about any non-violent conflict you have experienced; give us both sides of the story, and find out if you\u0026rsquo;re right, or you\u0026rsquo;re the asshole.","title":"Percentage Charts for the subreddit Am I the Asshole?"},{"content":"Greetings and welcome to my blog.\nThis blog is just mainly for me to focus my thoughts and it is a personal space for my projects.\nI am currently open to job opportunities.\nEmail - \u0026#32;\u0026#32;\r","permalink":"https://jianloong.github.io/about/","summary":"Greetings and welcome to my blog.\nThis blog is just mainly for me to focus my thoughts and it is a personal space for my projects.\nI am currently open to job opportunities.\nEmail - \u0026#32;\u0026#32;\r","title":"About"},{"content":"Email - \u0026#32;\u0026#32;\r","permalink":"https://jianloong.github.io/contact/","summary":"Email - \u0026#32;\u0026#32;\r","title":"Contact me"},{"content":"Sentiment Analysis This project is a collection of several lexicons used for sentiment analysis. Project is done purely using JavaScript with no node dependencies.\nIt features several well known lexicons for sentiment analysis and also real time evaluation of the text.\nIt can be found here. It is also on GitHub here\nReddit Crawler and Visualisation This project is a Reddit data-crawler that does not depend on the Reddit API. It is mean to run on a remote machine to run crawlers are predetermined time as well as building a simple dashboard based on on particular subreddit.\nThis project is inspired by the post here)\nIt can be found here. It is also on GitHub here\nFIT5032 - Internet Applications Development This project was done in the year 2018. I made major changes to both the readings and tutorial materials.\nThe previous offerings of the unit were not done using the MVC architecture and still used WebForms.\nHopefully, that since I am no longer working in Monash, the unit is still being updated and getting the love it deserves as an introduction unit.\nGeneticJS This project was an extension inspired by the post made here\nHowever during that, time, I was not able to dedicate enough of my time to complete the project. However, the baseline still exist and working, potentially including it in future projects.\n","permalink":"https://jianloong.github.io/projects/","summary":"Sentiment Analysis This project is a collection of several lexicons used for sentiment analysis. Project is done purely using JavaScript with no node dependencies.\nIt features several well known lexicons for sentiment analysis and also real time evaluation of the text.\nIt can be found here. It is also on GitHub here\nReddit Crawler and Visualisation This project is a Reddit data-crawler that does not depend on the Reddit API. It is mean to run on a remote machine to run crawlers are predetermined time as well as building a simple dashboard based on on particular subreddit.","title":"Projects"}]